"""
ç”Ÿæˆé—®é¢˜ä¸RAGæœç´¢ç»“æœå¯¹åº”çš„æ˜“è¯»æ–‡æ¡£
"""

import json
from pathlib import Path
from datetime import datetime


def generate_readable_mapping(analysis_file: str, output_file: str):
    """ç”Ÿæˆæ˜“è¯»çš„é—®é¢˜ä¸RAGæœç´¢ç»“æœå¯¹åº”æ–‡æ¡£"""
    
    with open(analysis_file, 'r', encoding='utf-8') as f:
        analysis = json.load(f)
    
    question = analysis['question']
    mapping = analysis['query_result_mapping']
    summary = analysis['summary']
    duplicates = analysis['duplicate_analysis']
    quality = analysis['quality_analysis']
    
    # ç”ŸæˆMarkdownæ–‡æ¡£
    doc_lines = [
        "# RAGæŸ¥è¯¢ç»“æœåˆ†ææŠ¥å‘Š",
        "",
        f"**é—®é¢˜**: {question}",
        f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "---",
        "",
        "## ğŸ“Š æ€»ä½“ç»Ÿè®¡",
        "",
        f"- **æ€»æŸ¥è¯¢æ•°**: {summary['total_queries']}",
        f"- **å”¯ä¸€chunkæ•°**: {summary['total_unique_chunks']}",
        f"- **é‡å¤chunkæ•°**: {summary['duplicate_chunks_count']}",
        "",
        "### æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ",
        "",
        "| æŸ¥è¯¢ç±»å‹ | æ•°é‡ |",
        "|---------|------|"
    ]
    
    for query_type, count in sorted(summary['queries_by_type'].items(), key=lambda x: x[1], reverse=True):
        doc_lines.append(f"| {query_type} | {count} |")
    
    doc_lines.extend([
        "",
        "### ç›¸ä¼¼åº¦ç»Ÿè®¡",
        "",
        f"- **æœ€ä½ç›¸ä¼¼åº¦**: {summary['similarity_stats']['min']:.4f}",
        f"- **æœ€é«˜ç›¸ä¼¼åº¦**: {summary['similarity_stats']['max']:.4f}",
        f"- **å¹³å‡ç›¸ä¼¼åº¦**: {summary['similarity_stats']['avg']:.4f}",
        "",
        "### æ¥æºåˆ†å¸ƒ",
        "",
        "| æ¥æº | å¼•ç”¨æ¬¡æ•° |",
        "|------|---------|"
    ])
    
    for source, count in sorted(summary['source_distribution'].items(), key=lambda x: x[1], reverse=True):
        doc_lines.append(f"| {source} | {count} |")
    
    doc_lines.extend([
        "",
        "### è´¨é‡åˆ†å¸ƒ",
        "",
        "| è´¨é‡ç­‰çº§ | æŸ¥è¯¢æ•° | è¯´æ˜ |",
        "|---------|--------|------|",
        f"| ä¼˜ç§€ | {summary['quality_distribution']['excellent']} | æœ€é«˜ç›¸ä¼¼åº¦ > 0.6 |",
        f"| è‰¯å¥½ | {summary['quality_distribution']['good']} | æœ€é«˜ç›¸ä¼¼åº¦ 0.4-0.6 |",
        f"| ä¸€èˆ¬ | {summary['quality_distribution']['fair']} | æœ€é«˜ç›¸ä¼¼åº¦ 0.3-0.4 |",
        f"| è¾ƒå·® | {summary['quality_distribution']['poor']} | æœ€é«˜ç›¸ä¼¼åº¦ < 0.3 æˆ–æ— ç»“æœ |",
        "",
        "---",
        "",
        "## ğŸ” è¯¦ç»†æŸ¥è¯¢ç»“æœæ˜ å°„",
        "",
        "### æŸ¥è¯¢ç»“æœè¯´æ˜",
        "",
        "- **num_results_retrieved**: æ¯æ¬¡æœç´¢è¿”å›çš„ç»“æœæ•°é‡",
        "- **similarity**: ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šç›¸å…³ï¼‰",
        "- **chunk_id**: æ–‡æ¡£å—çš„å”¯ä¸€æ ‡è¯†",
        "",
        "---",
        ""
    ])
    
    # æŒ‰æŸ¥è¯¢ç±»å‹åˆ†ç»„
    queries_by_type = {}
    for query in mapping:
        query_type = query['query_type']
        if query_type not in queries_by_type:
            queries_by_type[query_type] = []
        queries_by_type[query_type].append(query)
    
    # ç”Ÿæˆæ¯ä¸ªæŸ¥è¯¢çš„è¯¦ç»†ä¿¡æ¯
    for query_type, queries in sorted(queries_by_type.items()):
        doc_lines.extend([
            f"### {query_type}",
            ""
        ])
        
        for query in queries:
            card_name = query.get('card_name') or "N/A"
            doc_lines.extend([
                f"#### æŸ¥è¯¢ #{query['query_index']}: {query_type}",
                "",
                f"- **æŸ¥è¯¢æ–‡æœ¬**: `{query['query_text']}`",
                f"- **å…³è”å¡ç‰Œ**: {card_name}",
                f"- **è¿”å›ç»“æœæ•°**: {query['num_results_retrieved']}",
                f"- **å¤„ç†æ—¶é—´**: {query['latency_ms']}ms",
                "",
                "**æ£€ç´¢åˆ°çš„æ–‡æ¡£å—**:",
                ""
            ])
            
            if query['citations']:
                doc_lines.append("| æ¥æº | Chunk ID | ç›¸ä¼¼åº¦ |")
                doc_lines.append("|------|----------|--------|")
                for citation in query['citations']:
                    doc_lines.append(
                        f"| {citation['source']} | `{citation['chunk_id']}` | {citation['similarity']:.4f} |"
                    )
            else:
                doc_lines.append("*æ— ç»“æœ*")
            
            doc_lines.extend([
                "",
                f"**ç»“æœé¢„è§ˆ**: {query['result_text_preview']}",
                "",
                "---",
                ""
            ])
    
    # æ·»åŠ é‡å¤åˆ†æ
    doc_lines.extend([
        "## ğŸ”„ é‡å¤å†…å®¹åˆ†æ",
        "",
        f"å…±æœ‰ **{duplicates['total_duplicate_chunks']}** ä¸ªæ–‡æ¡£å—åœ¨å¤šä¸ªæŸ¥è¯¢ä¸­é‡å¤å‡ºç°ã€‚",
        "",
        "### é‡å¤æœ€å¤šçš„æ–‡æ¡£å—",
        ""
    ])
    
    # æ˜¾ç¤ºå‰10ä¸ªé‡å¤æœ€å¤šçš„chunk
    sorted_dups = sorted(
        duplicates['duplicates'].items(),
        key=lambda x: len(x[1]),
        reverse=True
    )[:10]
    
    for chunk_id, usages in sorted_dups:
        doc_lines.extend([
            f"#### `{chunk_id}`",
            "",
            f"å‡ºç°åœ¨ **{len(usages)}** ä¸ªæŸ¥è¯¢ä¸­:",
            ""
        ])
        
        for usage in usages[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            doc_lines.append(
                f"- **{usage['query_type']}** ({usage['card_name']}): "
                f"ç›¸ä¼¼åº¦ {usage['similarity']:.4f} - `{usage['query'][:60]}...`"
            )
        
        if len(usages) > 5:
            doc_lines.append(f"- ... è¿˜æœ‰ {len(usages) - 5} ä¸ªæŸ¥è¯¢")
        
        doc_lines.append("")
    
    # æ·»åŠ è´¨é‡åˆ†æ
    doc_lines.extend([
        "## ğŸ“ˆ è´¨é‡åˆ†æ",
        ""
    ])
    
    if quality['queries_with_no_results']:
        doc_lines.extend([
            "### âš ï¸ æ— ç»“æœçš„æŸ¥è¯¢",
            "",
            f"å…±æœ‰ {len(quality['queries_with_no_results'])} ä¸ªæŸ¥è¯¢æ²¡æœ‰è¿”å›ç»“æœ:",
            ""
        ])
        for q in quality['queries_with_no_results']:
            doc_lines.append(f"- **{q['type']}** ({q['card_name']}): `{q['query']}`")
        doc_lines.append("")
    
    if quality['queries_with_low_similarity']:
        doc_lines.extend([
            "### âš ï¸ ä½ç›¸ä¼¼åº¦æŸ¥è¯¢ï¼ˆç›¸ä¼¼åº¦ < 0.3ï¼‰",
            "",
            f"å…±æœ‰ {len(quality['queries_with_low_similarity'])} ä¸ªæŸ¥è¯¢ç›¸ä¼¼åº¦è¾ƒä½:",
            ""
        ])
        for q in quality['queries_with_low_similarity'][:10]:
            doc_lines.append(
                f"- **{q['type']}** ({q['card_name']}): "
                f"æœ€é«˜ç›¸ä¼¼åº¦ {q['max_similarity']:.4f} - `{q['query'][:60]}...`"
            )
        doc_lines.append("")
    
    if quality['queries_with_high_similarity']:
        doc_lines.extend([
            "### âœ… é«˜ç›¸ä¼¼åº¦æŸ¥è¯¢ï¼ˆç›¸ä¼¼åº¦ > 0.6ï¼‰",
            "",
            f"å…±æœ‰ {len(quality['queries_with_high_similarity'])} ä¸ªæŸ¥è¯¢ç›¸ä¼¼åº¦å¾ˆé«˜:",
            ""
        ])
        for q in quality['queries_with_high_similarity'][:10]:
            doc_lines.append(
                f"- **{q['type']}** ({q['card_name']}): "
                f"æœ€é«˜ç›¸ä¼¼åº¦ {q['max_similarity']:.4f} - `{q['query'][:60]}...`"
            )
        doc_lines.append("")
    
    # æ·»åŠ æ€»ç»“
    doc_lines.extend([
        "---",
        "",
        "## ğŸ“ æ€»ç»“",
        "",
        "### ä¸»è¦å‘ç°",
        "",
        "1. **æœç´¢æ•°æ®é‡**:",
        f"   - å¤§éƒ¨åˆ†æŸ¥è¯¢ï¼ˆ{summary['queries_by_type'].get('basic_meaning', 0) + summary['queries_by_type'].get('visual_description', 0)}ä¸ªï¼‰è¿”å›5ä¸ªç»“æœ",
        f"   - å¹³å‡ç›¸ä¼¼åº¦ {summary['similarity_stats']['avg']:.4f}ï¼Œè¯´æ˜æœç´¢ç»“æœç›¸å…³æ€§è¾ƒå¥½",
        "",
        "2. **é‡å¤æƒ…å†µ**:",
        f"   - æœ‰ {summary['duplicate_chunks_count']} ä¸ªæ–‡æ¡£å—åœ¨å¤šä¸ªæŸ¥è¯¢ä¸­é‡å¤å‡ºç°",
        "   - è¿™å¯èƒ½æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºåŒä¸€å¼ ç‰Œçš„ä¸åŒæŸ¥è¯¢ç±»å‹ï¼ˆå¦‚basic_meaningã€upright_meaningï¼‰å¯èƒ½æ£€ç´¢åˆ°ç›¸åŒçš„æ–‡æ¡£å—",
        "",
        "3. **æœç´¢ç»“æœè´¨é‡**:",
        f"   - {summary['quality_distribution']['excellent']} ä¸ªæŸ¥è¯¢è´¨é‡ä¼˜ç§€ï¼ˆç›¸ä¼¼åº¦>0.6ï¼‰",
        f"   - {summary['quality_distribution']['good']} ä¸ªæŸ¥è¯¢è´¨é‡è‰¯å¥½ï¼ˆç›¸ä¼¼åº¦0.4-0.6ï¼‰",
        f"   - åªæœ‰ {summary['quality_distribution']['poor']} ä¸ªæŸ¥è¯¢è´¨é‡è¾ƒå·®",
        "",
        "4. **æ¥æºåˆ†å¸ƒ**:",
        f"   - 78_degrees_of_wisdom.txt: {summary['source_distribution'].get('78_degrees_of_wisdom.txt', 0)} æ¬¡å¼•ç”¨",
        f"   - pkt.txt: {summary['source_distribution'].get('pkt.txt', 0)} æ¬¡å¼•ç”¨",
        "   - ä¸¤ä¸ªæ¥æºéƒ½æœ‰è¾ƒå¥½çš„è¦†ç›–",
        ""
    ])
    
    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(doc_lines))
    
    print(f"âœ… æ˜“è¯»æ–‡æ¡£å·²ç”Ÿæˆ: {output_file}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python generate_rag_mapping_doc.py <analysis_json_file> [output_md_file]")
        sys.exit(1)
    
    analysis_file = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else analysis_file.replace('.json', '_mapping.md')
    
    generate_readable_mapping(analysis_file, output_file)





